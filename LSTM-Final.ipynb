{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Scripts Using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _______________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - Function Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import seq2seq\n",
    "from collections import Counter\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "puncs = {\n",
    "            '--': '||long_dash||', \n",
    "             ',': '||comma||', \n",
    "             '.': '||period||',\n",
    "             '(': '||left_paren||',\n",
    "             ')': '||right_paren||',\n",
    "             ';': '||semicolon||',\n",
    "             '?': '||question||',\n",
    "            '\\n': '||new_line||',\n",
    "             '\"': '||double_quote||',\n",
    "             '!': '||exclamation||'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(input_text):\n",
    "    \"\"\"\n",
    "    1. Convert all charatcers to lower case\n",
    "    2. Assign unique tokens to punctuations\n",
    "    :param input_text: Raw script text\n",
    "    :return: Array of preprocessed words\n",
    "    \"\"\"\n",
    "    \n",
    "    new_text = input_text.lower()\n",
    "    \n",
    "    for token, replacement in puncs.items():\n",
    "        new_text = new_text.replace(token, \" \" + replacement + \" \")\n",
    "        \n",
    "    return new_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Create Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_dicts(text):\n",
    "    \"\"\"\n",
    "    Create dictionaries for index\n",
    "    :param input_text: Array of preprocessed words\n",
    "    :return int2word: Dictionary for index to word look up\n",
    "    :return word2int: Dictionary for word to index look up\n",
    "    \"\"\"\n",
    "    \n",
    "    c = Counter(text)\n",
    "    int2word, word2int = {}, {}\n",
    "    \n",
    "    for i, word in enumerate(c.items()):\n",
    "        \n",
    "        int2word[i] = word[0]\n",
    "        word2int[word[0]] = i\n",
    "        \n",
    "    return int2word, word2int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 - Mini Batch Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size=119, seq_length=20):\n",
    "    \"\"\"\n",
    "    Create dictionaries for index\n",
    "    :param input_text: Array pf preprocessed words\n",
    "    :return int2word: Dictionary for index to word look up\n",
    "    :return word2int: Dictionary for word to index look up\n",
    "    \"\"\"\n",
    "\n",
    "    int_text = int_text[16:]\n",
    "    n_batches = (len(int_text)-1)//(batch_size * seq_length)    \n",
    "    int_text = int_text[:n_batches * batch_size * seq_length + 1]\n",
    "    int_text_sequences = [int_text[i*seq_length:i*seq_length+seq_length] for i in range(0, n_batches * batch_size)]\n",
    "    int_text = int_text[1:]\n",
    "    int_text_targets = [int_text[i*seq_length:i*seq_length+seq_length] for i in range(0, n_batches * batch_size)]\n",
    "    output = []\n",
    "    for batch in range(n_batches):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for size in range(batch_size):\n",
    "            inputs.append(int_text_sequences[size * n_batches + batch])\n",
    "            targets.append(int_text_targets[size * n_batches + batch])\n",
    "        output.append([inputs, targets])\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 - LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LSTM(int2word, rnn_size=256, embed_dim=300):\n",
    "    \"\"\"\n",
    "    Implementation of LSTM and its training\n",
    "    :param int2word: Look up table for converting index to word\n",
    "    :param rnn_size: Size of each lstm cell\n",
    "    :param embed_dim: Size of the word embedding used before lstm\n",
    "    :return: Network variables used for training and testing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create Input and Output Placeholders\n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "\n",
    "    # Create LSTM Cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm, lstm])\n",
    "    initial_state = cell.zero_state(tf.shape(input_text)[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name= \"initial_state\")\n",
    "\n",
    "    # Create Embedding\n",
    "    inputs = tf.contrib.layers.embed_sequence(input_text, vocab_size=len(int2word), embed_dim=300)\n",
    "\n",
    "    # Pass Data Through Network\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name=\"final_state\")\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, len(int2word), activation_fn=None)\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Calculate Loss\n",
    "    cost = seq2seq.sequence_loss(logits, targets, tf.ones([tf.shape(input_text)[0], tf.shape(input_text)[1]]))\n",
    "\n",
    "    # Optimize\n",
    "    optimizer = tf.train.AdamOptimizer(0.01)\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    \n",
    "    return input_text, targets, cost, initial_state, final_state, train_op, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 - Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(sess, input_text, targets, initial_state, final_state, int_text, \n",
    "          batch_size=120, seq_length=20, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Perform network training\n",
    "    :param sess: Session which contains the network\n",
    "    :param batch_size: Size of each batch entering the network\n",
    "    :param seq_length: Length of each sequence entering the network\n",
    "    :param num_epochs: Number of iterations over the training set\n",
    "    :return initial_state: Initial cell state after training network\n",
    "    :return final_state: Final cell state after training the network\n",
    "    \"\"\"\n",
    "    \n",
    "    batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {input_text:x, targets:y, initial_state:state}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "            \n",
    "    \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % (show_every_n_batches) == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i, batch_i, len(batches), train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    #saver = tf.train.Saver()\n",
    "    #saver.save(sess, './save')\n",
    "    #print('Model Trained and Saved')\n",
    "    \n",
    "    return initial_state, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 - Calculate Probable Word From Network Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probability distribution outputted by network\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    word_int = np.random.choice(len(int_to_vocab), p=probabilities)\n",
    "    return int_to_vocab[word_int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8 - Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(sess, input_text, initial_state, final_state, seq_length=20, gen_length=200, start_char='moe_szyslak'):\n",
    "    \"\"\"\n",
    "    Test network by generating text from start_char, and feeding output back into network\n",
    "    :param sess: Session which contains the network\n",
    "    :param seq_length: Length of each sequence entering the network\n",
    "    :param gen_length: Number of words to be generated by the network\n",
    "    :return gen_sentences: Array of words generated by the network\n",
    "    \"\"\"\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [start_char + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[word2int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int2word)\n",
    "        gen_sentences.append(pred_word)\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 - Construct Sentences From Network Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_sentences(gen_sentences):\n",
    "    \"\"\"\n",
    "    Clean and process words generated by network and print them\n",
    "    :param gen_sentences: Array of words generated by the network\n",
    "    \"\"\"\n",
    "    script = \" \".join(gen_sentences)\n",
    "\n",
    "    for replacement, token in puncs.items():\n",
    "        script = script.replace(token, replacement)\n",
    "\n",
    "    script = script.replace(\" .\", \".\")\n",
    "    script = script.replace(\" ,\", \",\")\n",
    "    script = script.replace(\" !\", \"!\")\n",
    "    script = script.replace(\" ?\", \"?\")\n",
    "    script = script.replace(\"( \", \"(\")\n",
    "    script = script.replace(\" )\", \")\")\n",
    "    script = script.replace(' \\n', '\\n')\n",
    "    script = script.replace(\"  \", \" \")\n",
    "    script = script.replace(\" i \", \" I \")\n",
    "    script = script.replace(\"i'm\", \"I'm\")\n",
    "    script = script.replace(\"i'll\", \"I'll\")\n",
    "\n",
    "    for i, line in enumerate(script.split(\"\\n\")):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        else:\n",
    "            print(line[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _______________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 - Testing Model On Small Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Load Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./data/simpsons/moes_tavern_lines.txt')\n",
    "text = f.read()\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Display Sample Text From Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(text.split(\"\\n\")[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Smal Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Number of lines: 4257\n",
      "Number of characters 282\n"
     ]
    }
   ],
   "source": [
    "unique_words = len({word: None for word in text.split()})\n",
    "scene_count = len(text.split(\"\\n\\n\"))\n",
    "line_count = len([line for scene in text.split(\"\\n\\n\") for line in scene.split(\"\\n\")])\n",
    "character_count = len(Counter([line.split(\":\")[0] for line in text.split(\"\\n\")]))\n",
    "\n",
    "print(\"Number of unique words:\", unique_words)\n",
    "print(\"Number of scenes:\", scene_count)\n",
    "print(\"Number of lines:\", line_count)\n",
    "print(\"Number of characters\", character_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4- Prepare Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_text = preprocess(text)\n",
    "int2word, word2int = to_dicts(processed_text)\n",
    "int_text = [word2int[word] for word in processed_text]\n",
    "show_every_n_batches = get_batches(int_text, batch_size=120, seq_length=20).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 - Test Network On Small Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inp, out, cost, i_state, f_state, train_op, probs = LSTM(int2word, rnn_size=256, embed_dim=300)\n",
    "sess = tf.Session()\n",
    "%time i_state, f_state = train(sess, inp, out, i_state, f_state, int_text, batch_size=120, seq_length=20, num_epochs=200)\n",
    "gen_sentences = test(sess, inp, i_state, f_state, seq_length=20, gen_length=200, start_char='moe_szyslak')\n",
    "print_sentences(gen_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _______________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 - Testing With A Larger Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Load Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "large_data = []\n",
    "characters = []\n",
    "scene_count = 0\n",
    "line_count = 0\n",
    "\n",
    "with open('./data/simpsons.csv', newline='') as csvfile:\n",
    "    data = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    for i, row in enumerate(data):\n",
    "        if row[5] == 'false':\n",
    "            scene_count += 1\n",
    "        else:\n",
    "            line_count += 1\n",
    "        if i > 0 and len(row) > 9 and row[5] != 'false':\n",
    "            large_data.append(row[8].replace(\" \", \"_\")+\": \"+row[10])\n",
    "            characters.append(row[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Large Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 54307\n",
      "Number of scenes: 26158\n",
      "Number of lines: 132114\n",
      "Number of characters 6280\n"
     ]
    }
   ],
   "source": [
    "large_processed_text = preprocess(\" \".join(large_data))\n",
    "\n",
    "print(\"Number of unique words:\", len(Counter(large_processed_text)))\n",
    "print(\"Number of scenes:\", scene_count)\n",
    "print(\"Number of lines:\", line_count)\n",
    "print(\"Number of characters\", len(Counter(characters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Prepare Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_dicts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-904ef2609650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlarge_int2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlarge_word2int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlarge_processed_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlarge_int_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlarge_word2int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlarge_processed_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshow_every_n_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlarge_int_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m119\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_dicts' is not defined"
     ]
    }
   ],
   "source": [
    "large_int2word, large_word2int = to_dicts(large_processed_text)\n",
    "large_int_text = [large_word2int[word] for word in large_processed_text]\n",
    "show_every_n_batches = get_batches(large_int_text, batch_size=120, seq_length=20).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 - Test Network On Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/781   train_loss = 10.911\n",
      "Epoch   1 Batch    0/781   train_loss = 5.274\n",
      "Epoch   2 Batch    0/781   train_loss = 5.007\n",
      "Epoch   3 Batch    0/781   train_loss = 4.884\n",
      "Epoch   4 Batch    0/781   train_loss = 4.792\n",
      "Epoch   5 Batch    0/781   train_loss = 4.735\n",
      "Epoch   6 Batch    0/781   train_loss = 4.683\n",
      "Epoch   7 Batch    0/781   train_loss = 4.624\n",
      "Epoch   8 Batch    0/781   train_loss = 4.584\n",
      "Epoch   9 Batch    0/781   train_loss = 4.541\n",
      "Epoch  10 Batch    0/781   train_loss = 4.513\n",
      "Epoch  11 Batch    0/781   train_loss = 4.480\n",
      "Epoch  12 Batch    0/781   train_loss = 4.477\n",
      "Epoch  13 Batch    0/781   train_loss = 4.435\n",
      "Epoch  14 Batch    0/781   train_loss = 4.396\n",
      "Epoch  15 Batch    0/781   train_loss = 4.382\n",
      "Epoch  16 Batch    0/781   train_loss = 4.350\n",
      "Epoch  17 Batch    0/781   train_loss = 4.387\n",
      "Epoch  18 Batch    0/781   train_loss = 4.365\n",
      "Epoch  19 Batch    0/781   train_loss = 4.353\n",
      "Epoch  20 Batch    0/781   train_loss = 4.309\n",
      "Epoch  21 Batch    0/781   train_loss = 4.297\n",
      "Epoch  22 Batch    0/781   train_loss = 4.300\n",
      "Epoch  23 Batch    0/781   train_loss = 4.286\n",
      "Epoch  24 Batch    0/781   train_loss = 4.266\n",
      "Epoch  25 Batch    0/781   train_loss = 4.278\n",
      "Epoch  26 Batch    0/781   train_loss = 4.270\n",
      "Epoch  27 Batch    0/781   train_loss = 4.254\n",
      "Epoch  28 Batch    0/781   train_loss = 4.260\n",
      "Epoch  29 Batch    0/781   train_loss = 4.264\n",
      "Epoch  30 Batch    0/781   train_loss = 4.255\n",
      "Epoch  31 Batch    0/781   train_loss = 4.243\n",
      "Epoch  32 Batch    0/781   train_loss = 4.243\n",
      "Epoch  33 Batch    0/781   train_loss = 4.232\n",
      "Epoch  34 Batch    0/781   train_loss = 4.220\n",
      "Epoch  35 Batch    0/781   train_loss = 4.220\n",
      "Epoch  36 Batch    0/781   train_loss = 4.234\n",
      "Epoch  37 Batch    0/781   train_loss = 4.220\n",
      "Epoch  38 Batch    0/781   train_loss = 4.214\n",
      "Epoch  39 Batch    0/781   train_loss = 4.203\n",
      "Epoch  40 Batch    0/781   train_loss = 4.179\n",
      "Epoch  41 Batch    0/781   train_loss = 4.196\n",
      "Epoch  42 Batch    0/781   train_loss = 4.172\n",
      "Epoch  43 Batch    0/781   train_loss = 4.199\n",
      "Epoch  44 Batch    0/781   train_loss = 4.172\n",
      "Epoch  45 Batch    0/781   train_loss = 4.176\n",
      "Epoch  46 Batch    0/781   train_loss = 4.156\n",
      "Epoch  47 Batch    0/781   train_loss = 4.130\n",
      "Epoch  48 Batch    0/781   train_loss = 4.138\n",
      "Epoch  49 Batch    0/781   train_loss = 4.162\n",
      "Epoch  50 Batch    0/781   train_loss = 4.152\n",
      "Epoch  51 Batch    0/781   train_loss = 4.152\n",
      "Epoch  52 Batch    0/781   train_loss = 4.150\n",
      "Epoch  53 Batch    0/781   train_loss = 4.123\n",
      "Epoch  54 Batch    0/781   train_loss = 4.138\n",
      "Epoch  55 Batch    0/781   train_loss = 4.115\n",
      "Epoch  56 Batch    0/781   train_loss = 4.133\n",
      "Epoch  57 Batch    0/781   train_loss = 4.138\n",
      "Epoch  58 Batch    0/781   train_loss = 4.131\n",
      "Epoch  59 Batch    0/781   train_loss = 4.122\n",
      "Epoch  60 Batch    0/781   train_loss = 4.145\n",
      "Epoch  61 Batch    0/781   train_loss = 4.138\n",
      "Epoch  62 Batch    0/781   train_loss = 4.152\n",
      "Epoch  63 Batch    0/781   train_loss = 4.123\n",
      "Epoch  64 Batch    0/781   train_loss = 4.128\n",
      "Epoch  65 Batch    0/781   train_loss = 4.125\n",
      "Epoch  66 Batch    0/781   train_loss = 4.112\n",
      "Epoch  67 Batch    0/781   train_loss = 4.105\n",
      "Epoch  68 Batch    0/781   train_loss = 4.103\n",
      "Epoch  69 Batch    0/781   train_loss = 4.105\n",
      "Epoch  70 Batch    0/781   train_loss = 4.130\n",
      "Epoch  71 Batch    0/781   train_loss = 4.135\n",
      "Epoch  72 Batch    0/781   train_loss = 4.115\n",
      "Epoch  73 Batch    0/781   train_loss = 4.118\n",
      "Epoch  74 Batch    0/781   train_loss = 4.096\n",
      "Epoch  75 Batch    0/781   train_loss = 4.107\n",
      "Epoch  76 Batch    0/781   train_loss = 4.108\n",
      "Epoch  77 Batch    0/781   train_loss = 4.126\n",
      "Epoch  78 Batch    0/781   train_loss = 4.127\n",
      "Epoch  79 Batch    0/781   train_loss = 4.137\n",
      "Epoch  80 Batch    0/781   train_loss = 4.113\n",
      "Epoch  81 Batch    0/781   train_loss = 4.149\n",
      "Epoch  82 Batch    0/781   train_loss = 4.132\n",
      "Epoch  83 Batch    0/781   train_loss = 4.112\n",
      "Epoch  84 Batch    0/781   train_loss = 4.134\n",
      "Epoch  85 Batch    0/781   train_loss = 4.149\n",
      "Epoch  86 Batch    0/781   train_loss = 4.150\n",
      "Epoch  87 Batch    0/781   train_loss = 4.163\n",
      "Epoch  88 Batch    0/781   train_loss = 4.141\n",
      "Epoch  89 Batch    0/781   train_loss = 4.122\n",
      "Epoch  90 Batch    0/781   train_loss = 4.121\n",
      "Epoch  91 Batch    0/781   train_loss = 4.121\n",
      "Epoch  92 Batch    0/781   train_loss = 4.105\n",
      "Epoch  93 Batch    0/781   train_loss = 4.138\n",
      "Epoch  94 Batch    0/781   train_loss = 4.122\n",
      "Epoch  95 Batch    0/781   train_loss = 4.120\n",
      "Epoch  96 Batch    0/781   train_loss = 4.149\n",
      "Epoch  97 Batch    0/781   train_loss = 4.122\n",
      "Epoch  98 Batch    0/781   train_loss = 4.135\n",
      "Epoch  99 Batch    0/781   train_loss = 4.116\n",
      "Epoch 100 Batch    0/781   train_loss = 4.128\n",
      "Epoch 101 Batch    0/781   train_loss = 4.116\n",
      "Epoch 102 Batch    0/781   train_loss = 4.119\n",
      "Epoch 103 Batch    0/781   train_loss = 4.102\n",
      "Epoch 104 Batch    0/781   train_loss = 4.111\n",
      "Epoch 105 Batch    0/781   train_loss = 4.123\n",
      "Epoch 106 Batch    0/781   train_loss = 4.131\n",
      "Epoch 107 Batch    0/781   train_loss = 4.140\n",
      "Epoch 108 Batch    0/781   train_loss = 4.131\n",
      "Epoch 109 Batch    0/781   train_loss = 4.126\n",
      "Epoch 110 Batch    0/781   train_loss = 4.115\n",
      "Epoch 111 Batch    0/781   train_loss = 4.119\n",
      "Epoch 112 Batch    0/781   train_loss = 4.131\n",
      "Epoch 113 Batch    0/781   train_loss = 4.118\n",
      "Epoch 114 Batch    0/781   train_loss = 4.125\n",
      "Epoch 115 Batch    0/781   train_loss = 4.117\n",
      "Epoch 116 Batch    0/781   train_loss = 4.116\n",
      "Epoch 117 Batch    0/781   train_loss = 4.126\n",
      "Epoch 118 Batch    0/781   train_loss = 4.110\n",
      "Epoch 119 Batch    0/781   train_loss = 4.107\n",
      "Epoch 120 Batch    0/781   train_loss = 4.139\n",
      "Epoch 121 Batch    0/781   train_loss = 4.113\n",
      "Epoch 122 Batch    0/781   train_loss = 4.125\n",
      "Epoch 123 Batch    0/781   train_loss = 4.123\n",
      "Epoch 124 Batch    0/781   train_loss = 4.128\n",
      "Epoch 125 Batch    0/781   train_loss = 4.140\n",
      "Epoch 126 Batch    0/781   train_loss = 4.144\n",
      "Epoch 127 Batch    0/781   train_loss = 4.154\n",
      "Epoch 128 Batch    0/781   train_loss = 4.152\n",
      "Epoch 129 Batch    0/781   train_loss = 4.157\n",
      "Epoch 130 Batch    0/781   train_loss = 4.138\n",
      "Epoch 131 Batch    0/781   train_loss = 4.137\n",
      "Epoch 132 Batch    0/781   train_loss = 4.134\n",
      "Epoch 133 Batch    0/781   train_loss = 4.143\n",
      "Epoch 134 Batch    0/781   train_loss = 4.132\n",
      "Epoch 135 Batch    0/781   train_loss = 4.129\n",
      "Epoch 136 Batch    0/781   train_loss = 4.129\n",
      "Epoch 137 Batch    0/781   train_loss = 4.157\n",
      "Epoch 138 Batch    0/781   train_loss = 4.122\n",
      "Epoch 139 Batch    0/781   train_loss = 4.158\n",
      "Epoch 140 Batch    0/781   train_loss = 4.155\n",
      "Epoch 141 Batch    0/781   train_loss = 4.168\n",
      "Epoch 142 Batch    0/781   train_loss = 4.144\n",
      "Epoch 143 Batch    0/781   train_loss = 4.131\n",
      "Epoch 144 Batch    0/781   train_loss = 4.127\n",
      "Epoch 145 Batch    0/781   train_loss = 4.124\n",
      "Epoch 146 Batch    0/781   train_loss = 4.117\n",
      "Epoch 147 Batch    0/781   train_loss = 4.125\n",
      "Epoch 148 Batch    0/781   train_loss = 4.131\n",
      "Epoch 149 Batch    0/781   train_loss = 4.125\n",
      "Epoch 150 Batch    0/781   train_loss = 4.137\n",
      "Epoch 151 Batch    0/781   train_loss = 4.145\n",
      "Epoch 152 Batch    0/781   train_loss = 4.159\n",
      "Epoch 153 Batch    0/781   train_loss = 4.135\n",
      "Epoch 154 Batch    0/781   train_loss = 4.142\n",
      "Epoch 155 Batch    0/781   train_loss = 4.134\n",
      "Epoch 156 Batch    0/781   train_loss = 4.125\n",
      "Epoch 157 Batch    0/781   train_loss = 4.119\n",
      "Epoch 158 Batch    0/781   train_loss = 4.109\n",
      "Epoch 159 Batch    0/781   train_loss = 4.115\n",
      "Epoch 160 Batch    0/781   train_loss = 4.118\n",
      "Epoch 161 Batch    0/781   train_loss = 4.111\n",
      "Epoch 162 Batch    0/781   train_loss = 4.107\n",
      "Epoch 163 Batch    0/781   train_loss = 4.126\n",
      "Epoch 164 Batch    0/781   train_loss = 4.096\n",
      "Epoch 165 Batch    0/781   train_loss = 4.095\n",
      "Epoch 166 Batch    0/781   train_loss = 4.123\n",
      "Epoch 167 Batch    0/781   train_loss = 4.105\n",
      "Epoch 168 Batch    0/781   train_loss = 4.115\n",
      "Epoch 169 Batch    0/781   train_loss = 4.117\n",
      "Epoch 170 Batch    0/781   train_loss = 4.125\n",
      "Epoch 171 Batch    0/781   train_loss = 4.127\n",
      "Epoch 172 Batch    0/781   train_loss = 4.150\n",
      "Epoch 173 Batch    0/781   train_loss = 4.153\n",
      "Epoch 174 Batch    0/781   train_loss = 4.111\n",
      "Epoch 175 Batch    0/781   train_loss = 4.114\n",
      "Epoch 176 Batch    0/781   train_loss = 4.119\n",
      "Epoch 177 Batch    0/781   train_loss = 4.140\n",
      "Epoch 178 Batch    0/781   train_loss = 4.132\n",
      "Epoch 179 Batch    0/781   train_loss = 4.140\n",
      "Epoch 180 Batch    0/781   train_loss = 4.155\n",
      "Epoch 181 Batch    0/781   train_loss = 4.133\n",
      "Epoch 182 Batch    0/781   train_loss = 4.165\n",
      "Epoch 183 Batch    0/781   train_loss = 4.200\n",
      "Epoch 184 Batch    0/781   train_loss = 4.184\n",
      "Epoch 185 Batch    0/781   train_loss = 4.144\n",
      "Epoch 186 Batch    0/781   train_loss = 4.149\n",
      "Epoch 187 Batch    0/781   train_loss = 4.129\n",
      "Epoch 188 Batch    0/781   train_loss = 4.134\n",
      "Epoch 189 Batch    0/781   train_loss = 4.105\n",
      "Epoch 190 Batch    0/781   train_loss = 4.124\n",
      "Epoch 191 Batch    0/781   train_loss = 4.140\n",
      "Epoch 192 Batch    0/781   train_loss = 4.140\n",
      "Epoch 193 Batch    0/781   train_loss = 4.113\n",
      "Epoch 194 Batch    0/781   train_loss = 4.135\n",
      "Epoch 195 Batch    0/781   train_loss = 4.115\n",
      "Epoch 196 Batch    0/781   train_loss = 4.108\n",
      "Epoch 197 Batch    0/781   train_loss = 4.141\n",
      "Epoch 198 Batch    0/781   train_loss = 4.124\n",
      "Epoch 199 Batch    0/781   train_loss = 4.189\n",
      "Epoch 200 Batch    0/781   train_loss = 4.291\n",
      "Epoch 201 Batch    0/781   train_loss = 4.171\n",
      "Epoch 202 Batch    0/781   train_loss = 4.153\n",
      "Epoch 203 Batch    0/781   train_loss = 4.153\n",
      "Epoch 204 Batch    0/781   train_loss = 4.132\n",
      "Epoch 205 Batch    0/781   train_loss = 4.117\n",
      "Epoch 206 Batch    0/781   train_loss = 4.147\n",
      "Epoch 207 Batch    0/781   train_loss = 4.143\n",
      "Epoch 208 Batch    0/781   train_loss = 4.117\n",
      "Epoch 209 Batch    0/781   train_loss = 4.106\n",
      "Epoch 210 Batch    0/781   train_loss = 4.096\n",
      "Epoch 211 Batch    0/781   train_loss = 4.155\n",
      "Epoch 212 Batch    0/781   train_loss = 4.097\n",
      "Epoch 213 Batch    0/781   train_loss = 4.174\n",
      "Epoch 214 Batch    0/781   train_loss = 4.125\n",
      "Epoch 215 Batch    0/781   train_loss = 4.109\n",
      "Epoch 216 Batch    0/781   train_loss = 4.116\n",
      "Epoch 217 Batch    0/781   train_loss = 4.088\n",
      "Epoch 218 Batch    0/781   train_loss = 4.091\n",
      "Epoch 219 Batch    0/781   train_loss = 4.093\n",
      "Epoch 220 Batch    0/781   train_loss = 4.088\n",
      "Epoch 221 Batch    0/781   train_loss = 4.075\n",
      "Epoch 222 Batch    0/781   train_loss = 4.104\n",
      "Epoch 223 Batch    0/781   train_loss = 4.118\n",
      "Epoch 224 Batch    0/781   train_loss = 4.117\n",
      "Epoch 225 Batch    0/781   train_loss = 4.107\n",
      "Epoch 226 Batch    0/781   train_loss = 4.102\n",
      "Epoch 227 Batch    0/781   train_loss = 4.119\n",
      "Epoch 228 Batch    0/781   train_loss = 4.092\n",
      "Epoch 229 Batch    0/781   train_loss = 4.093\n",
      "Epoch 230 Batch    0/781   train_loss = 4.279\n",
      "Epoch 231 Batch    0/781   train_loss = 4.220\n",
      "Epoch 232 Batch    0/781   train_loss = 4.172\n",
      "Epoch 233 Batch    0/781   train_loss = 4.168\n",
      "Epoch 234 Batch    0/781   train_loss = 4.151\n",
      "Epoch 235 Batch    0/781   train_loss = 4.147\n",
      "Epoch 236 Batch    0/781   train_loss = 4.156\n",
      "Epoch 237 Batch    0/781   train_loss = 4.138\n",
      "Epoch 238 Batch    0/781   train_loss = 4.115\n",
      "Epoch 239 Batch    0/781   train_loss = 4.096\n",
      "Epoch 240 Batch    0/781   train_loss = 4.095\n",
      "Epoch 241 Batch    0/781   train_loss = 4.085\n",
      "Epoch 242 Batch    0/781   train_loss = 4.105\n",
      "Epoch 243 Batch    0/781   train_loss = 4.105\n",
      "Epoch 244 Batch    0/781   train_loss = 4.088\n",
      "Epoch 245 Batch    0/781   train_loss = 4.092\n",
      "Epoch 246 Batch    0/781   train_loss = 4.141\n",
      "Epoch 247 Batch    0/781   train_loss = 4.105\n",
      "Epoch 248 Batch    0/781   train_loss = 4.091\n",
      "Epoch 249 Batch    0/781   train_loss = 4.083\n",
      "Epoch 250 Batch    0/781   train_loss = 4.086\n",
      "Epoch 251 Batch    0/781   train_loss = 4.088\n",
      "Epoch 252 Batch    0/781   train_loss = 4.083\n",
      "Epoch 253 Batch    0/781   train_loss = 4.117\n",
      "Epoch 254 Batch    0/781   train_loss = 4.389\n",
      "Epoch 255 Batch    0/781   train_loss = 4.244\n",
      "Epoch 256 Batch    0/781   train_loss = 4.243\n",
      "Epoch 257 Batch    0/781   train_loss = 4.165\n",
      "Epoch 258 Batch    0/781   train_loss = 4.116\n",
      "Epoch 259 Batch    0/781   train_loss = 4.140\n",
      "Epoch 260 Batch    0/781   train_loss = 4.128\n",
      "Epoch 261 Batch    0/781   train_loss = 4.119\n",
      "Epoch 262 Batch    0/781   train_loss = 4.095\n",
      "Epoch 263 Batch    0/781   train_loss = 4.085\n",
      "Epoch 264 Batch    0/781   train_loss = 4.098\n",
      "Epoch 265 Batch    0/781   train_loss = 4.085\n",
      "Epoch 266 Batch    0/781   train_loss = 4.095\n",
      "Epoch 267 Batch    0/781   train_loss = 4.095\n",
      "Epoch 268 Batch    0/781   train_loss = 4.097\n",
      "Epoch 269 Batch    0/781   train_loss = 4.102\n",
      "Epoch 270 Batch    0/781   train_loss = 4.171\n",
      "Epoch 271 Batch    0/781   train_loss = 4.148\n",
      "Epoch 272 Batch    0/781   train_loss = 4.155\n",
      "Epoch 273 Batch    0/781   train_loss = 4.165\n",
      "Epoch 274 Batch    0/781   train_loss = 4.147\n",
      "Epoch 275 Batch    0/781   train_loss = 4.139\n",
      "Epoch 276 Batch    0/781   train_loss = 4.111\n",
      "Epoch 277 Batch    0/781   train_loss = 4.159\n",
      "Epoch 278 Batch    0/781   train_loss = 4.178\n",
      "Epoch 279 Batch    0/781   train_loss = 4.115\n",
      "Epoch 280 Batch    0/781   train_loss = 4.121\n",
      "Epoch 281 Batch    0/781   train_loss = 4.131\n",
      "Epoch 282 Batch    0/781   train_loss = 4.117\n",
      "Epoch 283 Batch    0/781   train_loss = 4.121\n",
      "Epoch 284 Batch    0/781   train_loss = 4.080\n",
      "Epoch 285 Batch    0/781   train_loss = 4.102\n",
      "Epoch 286 Batch    0/781   train_loss = 4.097\n",
      "Epoch 287 Batch    0/781   train_loss = 4.098\n",
      "Epoch 288 Batch    0/781   train_loss = 4.087\n",
      "Epoch 289 Batch    0/781   train_loss = 4.073\n",
      "Epoch 290 Batch    0/781   train_loss = 4.082\n",
      "Epoch 291 Batch    0/781   train_loss = 4.082\n",
      "Epoch 292 Batch    0/781   train_loss = 4.078\n",
      "Epoch 293 Batch    0/781   train_loss = 4.372\n",
      "Epoch 294 Batch    0/781   train_loss = 4.152\n",
      "Epoch 295 Batch    0/781   train_loss = 4.298\n",
      "Epoch 296 Batch    0/781   train_loss = 4.189\n",
      "Epoch 297 Batch    0/781   train_loss = 4.422\n",
      "Epoch 298 Batch    0/781   train_loss = 4.225\n",
      "Epoch 299 Batch    0/781   train_loss = 4.743\n",
      "Epoch 300 Batch    0/781   train_loss = 4.371\n",
      "Epoch 301 Batch    0/781   train_loss = 4.510\n",
      "Epoch 302 Batch    0/781   train_loss = 4.464\n",
      "Epoch 303 Batch    0/781   train_loss = 4.256\n",
      "Epoch 304 Batch    0/781   train_loss = 4.217\n",
      "Epoch 305 Batch    0/781   train_loss = 4.214\n",
      "Epoch 306 Batch    0/781   train_loss = 4.219\n",
      "Epoch 307 Batch    0/781   train_loss = 4.172\n",
      "Epoch 308 Batch    0/781   train_loss = 4.657\n",
      "Epoch 309 Batch    0/781   train_loss = 4.357\n",
      "Epoch 310 Batch    0/781   train_loss = 4.507\n",
      "Epoch 311 Batch    0/781   train_loss = 4.401\n",
      "Epoch 312 Batch    0/781   train_loss = 4.206\n",
      "Epoch 313 Batch    0/781   train_loss = 4.227\n",
      "Epoch 314 Batch    0/781   train_loss = 4.207\n",
      "Epoch 315 Batch    0/781   train_loss = 4.191\n",
      "Epoch 316 Batch    0/781   train_loss = 4.339\n",
      "Epoch 317 Batch    0/781   train_loss = 4.348\n",
      "Epoch 318 Batch    0/781   train_loss = 4.314\n",
      "Epoch 319 Batch    0/781   train_loss = 4.301\n",
      "Epoch 320 Batch    0/781   train_loss = 4.243\n",
      "Epoch 321 Batch    0/781   train_loss = 4.177\n",
      "Epoch 322 Batch    0/781   train_loss = 4.216\n",
      "Epoch 323 Batch    0/781   train_loss = 4.170\n",
      "Epoch 324 Batch    0/781   train_loss = 4.137\n",
      "Epoch 325 Batch    0/781   train_loss = 4.295\n",
      "Epoch 326 Batch    0/781   train_loss = 4.200\n",
      "Epoch 327 Batch    0/781   train_loss = 4.522\n",
      "Epoch 328 Batch    0/781   train_loss = 4.318\n",
      "Epoch 329 Batch    0/781   train_loss = 4.193\n",
      "Epoch 330 Batch    0/781   train_loss = 4.673\n",
      "Epoch 331 Batch    0/781   train_loss = 4.601\n",
      "Epoch 332 Batch    0/781   train_loss = 4.234\n",
      "Epoch 333 Batch    0/781   train_loss = 4.200\n",
      "Epoch 334 Batch    0/781   train_loss = 4.308\n",
      "Epoch 335 Batch    0/781   train_loss = 4.610\n",
      "Epoch 336 Batch    0/781   train_loss = 4.203\n",
      "Epoch 337 Batch    0/781   train_loss = 4.332\n",
      "Epoch 338 Batch    0/781   train_loss = 4.155\n",
      "Epoch 339 Batch    0/781   train_loss = 4.209\n",
      "Epoch 340 Batch    0/781   train_loss = 4.367\n",
      "Epoch 341 Batch    0/781   train_loss = 4.153\n",
      "Epoch 342 Batch    0/781   train_loss = 4.133\n",
      "Epoch 343 Batch    0/781   train_loss = 4.111\n",
      "Epoch 344 Batch    0/781   train_loss = 4.112\n",
      "Epoch 345 Batch    0/781   train_loss = 4.086\n",
      "Epoch 346 Batch    0/781   train_loss = 4.106\n",
      "Epoch 347 Batch    0/781   train_loss = 4.091\n",
      "Epoch 348 Batch    0/781   train_loss = 4.115\n",
      "Epoch 349 Batch    0/781   train_loss = 4.107\n",
      "Epoch 350 Batch    0/781   train_loss = 4.109\n",
      "Epoch 351 Batch    0/781   train_loss = 4.118\n",
      "Epoch 352 Batch    0/781   train_loss = 4.107\n",
      "Epoch 353 Batch    0/781   train_loss = 4.112\n",
      "Epoch 354 Batch    0/781   train_loss = 4.113\n",
      "Epoch 355 Batch    0/781   train_loss = 4.102\n",
      "Epoch 356 Batch    0/781   train_loss = 4.114\n",
      "Epoch 357 Batch    0/781   train_loss = 4.086\n",
      "Epoch 358 Batch    0/781   train_loss = 4.101\n",
      "Epoch 359 Batch    0/781   train_loss = 4.120\n",
      "Epoch 360 Batch    0/781   train_loss = 4.118\n",
      "Epoch 361 Batch    0/781   train_loss = 4.130\n",
      "Epoch 362 Batch    0/781   train_loss = 4.101\n",
      "Epoch 363 Batch    0/781   train_loss = 4.098\n",
      "Epoch 364 Batch    0/781   train_loss = 4.092\n",
      "Epoch 365 Batch    0/781   train_loss = 4.110\n",
      "Epoch 366 Batch    0/781   train_loss = 4.092\n",
      "Epoch 367 Batch    0/781   train_loss = 4.090\n",
      "Epoch 368 Batch    0/781   train_loss = 4.076\n",
      "Epoch 369 Batch    0/781   train_loss = 4.102\n",
      "Epoch 370 Batch    0/781   train_loss = 4.299\n",
      "Epoch 371 Batch    0/781   train_loss = 4.198\n",
      "Epoch 372 Batch    0/781   train_loss = 4.176\n",
      "Epoch 373 Batch    0/781   train_loss = 4.141\n",
      "Epoch 374 Batch    0/781   train_loss = 4.212\n",
      "Epoch 375 Batch    0/781   train_loss = 4.198\n",
      "Epoch 376 Batch    0/781   train_loss = 4.152\n",
      "Epoch 377 Batch    0/781   train_loss = 4.144\n",
      "Epoch 378 Batch    0/781   train_loss = 4.118\n",
      "Epoch 379 Batch    0/781   train_loss = 4.119\n",
      "Epoch 380 Batch    0/781   train_loss = 4.105\n",
      "Epoch 381 Batch    0/781   train_loss = 4.111\n",
      "Epoch 382 Batch    0/781   train_loss = 4.104\n",
      "Epoch 383 Batch    0/781   train_loss = 4.096\n",
      "Epoch 384 Batch    0/781   train_loss = 4.147\n",
      "Epoch 385 Batch    0/781   train_loss = 4.111\n",
      "Epoch 386 Batch    0/781   train_loss = 4.112\n",
      "Epoch 387 Batch    0/781   train_loss = 4.121\n",
      "Epoch 388 Batch    0/781   train_loss = 4.087\n",
      "Epoch 389 Batch    0/781   train_loss = 4.098\n",
      "Epoch 390 Batch    0/781   train_loss = 4.206\n",
      "Epoch 391 Batch    0/781   train_loss = 4.140\n",
      "Epoch 392 Batch    0/781   train_loss = 4.317\n",
      "Epoch 393 Batch    0/781   train_loss = 4.165\n",
      "Epoch 394 Batch    0/781   train_loss = 4.131\n",
      "Epoch 395 Batch    0/781   train_loss = 4.156\n",
      "Epoch 396 Batch    0/781   train_loss = 4.127\n",
      "Epoch 397 Batch    0/781   train_loss = 4.098\n",
      "Epoch 398 Batch    0/781   train_loss = 4.120\n",
      "Epoch 399 Batch    0/781   train_loss = 4.110\n",
      "Epoch 400 Batch    0/781   train_loss = 4.118\n",
      "Epoch 401 Batch    0/781   train_loss = 4.101\n",
      "Epoch 402 Batch    0/781   train_loss = 4.098\n",
      "Epoch 403 Batch    0/781   train_loss = 4.113\n",
      "Epoch 404 Batch    0/781   train_loss = 4.102\n",
      "Epoch 405 Batch    0/781   train_loss = 4.176\n",
      "Epoch 406 Batch    0/781   train_loss = 4.149\n",
      "Epoch 407 Batch    0/781   train_loss = 4.116\n",
      "Epoch 408 Batch    0/781   train_loss = 4.097\n",
      "Epoch 409 Batch    0/781   train_loss = 4.106\n",
      "Epoch 410 Batch    0/781   train_loss = 4.127\n",
      "Epoch 411 Batch    0/781   train_loss = 4.111\n",
      "Epoch 412 Batch    0/781   train_loss = 4.120\n",
      "Epoch 413 Batch    0/781   train_loss = 4.099\n",
      "Epoch 414 Batch    0/781   train_loss = 4.129\n",
      "Epoch 415 Batch    0/781   train_loss = 4.371\n",
      "Epoch 416 Batch    0/781   train_loss = 4.090\n",
      "Epoch 417 Batch    0/781   train_loss = 4.093\n",
      "Epoch 418 Batch    0/781   train_loss = 4.083\n",
      "Epoch 419 Batch    0/781   train_loss = 4.088\n",
      "Epoch 420 Batch    0/781   train_loss = 4.082\n",
      "Epoch 421 Batch    0/781   train_loss = 4.260\n",
      "Epoch 422 Batch    0/781   train_loss = 4.177\n",
      "Epoch 423 Batch    0/781   train_loss = 4.182\n",
      "Epoch 424 Batch    0/781   train_loss = 4.161\n",
      "Epoch 425 Batch    0/781   train_loss = 4.138\n",
      "Epoch 426 Batch    0/781   train_loss = 4.127\n",
      "Epoch 427 Batch    0/781   train_loss = 4.120\n",
      "Epoch 428 Batch    0/781   train_loss = 4.268\n",
      "Epoch 429 Batch    0/781   train_loss = 4.118\n",
      "Epoch 430 Batch    0/781   train_loss = 4.123\n",
      "Epoch 431 Batch    0/781   train_loss = 4.140\n",
      "Epoch 432 Batch    0/781   train_loss = 4.127\n",
      "Epoch 433 Batch    0/781   train_loss = 4.119\n",
      "Epoch 434 Batch    0/781   train_loss = 4.138\n",
      "Epoch 435 Batch    0/781   train_loss = 4.139\n",
      "Epoch 436 Batch    0/781   train_loss = 4.115\n",
      "Epoch 437 Batch    0/781   train_loss = 4.131\n",
      "Epoch 438 Batch    0/781   train_loss = 4.121\n",
      "Epoch 439 Batch    0/781   train_loss = 4.106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d95a19a0e8b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlarge_int2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time i_state, f_state = train(sess, inp, out, i_state, f_state, large_int_text, batch_size=119, seq_length=20, num_epochs=1000)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgen_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_char\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'moe_szyslak'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thund3rwar/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thund3rwar/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/thund3rwar/anaconda3/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thund3rwar/anaconda3/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-906f57806cd5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, input_text, targets, initial_state, final_state, int_text, batch_size, seq_length, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thund3rwar/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thund3rwar/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thund3rwar/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/thund3rwar/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thund3rwar/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inp, out, cost, i_state, f_state, train_op, probs = LSTM(large_int2word, rnn_size=512, embed_dim=600)\n",
    "sess = tf.Session()\n",
    "%time i_state, f_state = train(sess, inp, out, i_state, f_state, large_int_text, batch_size=120, \n",
    "                               seq_length=20, num_epochs=10)\n",
    "gen_sentences = test(sess, inp, i_state, f_state, seq_length=20, gen_length=200, start_char='moe_szyslak')\n",
    "print_sentences(gen_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
